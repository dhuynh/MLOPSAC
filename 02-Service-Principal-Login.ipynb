{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ML Pipeline at DevOps Build Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to make some modifications to the previous notebook (MLPipeline_MNIST) so that Azure DevOps Build Pipeline can generate a new ML Pipeline every time the master branch of the GitHub repo is changed.\n",
    "\n",
    "This is an important step to build a fully automated CI/CD pipeline for our ML project. So the scenario works like this:\n",
    "\n",
    "As a new code hits the master branch (this time we like to trigger the build Pipeline at the CI \"merge into the Master branch\") that hosts our code for the training pipeline, we like to execute the code to generate a new ML Pipeline with the new code. The ML Pipeline then generates a new ML model. The ML models is evaluated and if the accuracy is higher than the existing model, it is pushed into production.\n",
    "\n",
    "One major difference in this scenario is that we have to generate the ML Pipeline from the Ubuntu computer within Azure DevOps. That computer doesn't have access to our Azure's subscription and also we don't want to manually go through the authentication process. We want this to be automatic. Therefore, we need to create a mechanisim that the machine can log in in absence of us to access our Azure environment and in particular our Azure Workspace.\n",
    "\n",
    "One way to do this is to create a Service Principal. This is a \"special\" user is designed to let applications authenticate into Azure. So first we need to create a Service Principal.\n",
    "\n",
    "#### Create the Service Principal using the Portal\n",
    "The steps are provided here: https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal\n",
    "\n",
    "#### Create the Service Principal using the Command Line\n",
    "Another way to create the SP is to use the command line. From a terminal (launched from the Notebook or otherwise through SSH):\n",
    "```\n",
    "az login --use-device-code\n",
    "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code **SOME_CODE** to authenticate.\n",
    "```\n",
    "Simply at this point go to the URL, paste the code and your command line session will be authenticated. Then, to create the SP, type the following command:\n",
    "```\n",
    "azureuser@demovm17a8189392c2d:~/cloudfiles/code/omartin/MLOPSAC$ az ad sp create-for-rbac\n",
    "Creating a role assignment under the scope of \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "  Retrying role assignment creation: 1/36\n",
    "{\n",
    "  \"appId\": \"app_id\",\n",
    "  \"displayName\": \"azure-cli-2020-01-06-17-44-32\",\n",
    "  \"name\": \"http://azure-cli-2020-01-06-17-44-32\",\n",
    "  \"password\": \"password = secret, synonyms...\",\n",
    "  \"tenant\": \"tenant...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Save the following pieces of information: **Application ID**, **Tenant ID**, **Secret Key** and replace them in the code below:\n",
    "\n",
    "1. Create an Azure Active Directory application\n",
    "2. Assign the application to a role\n",
    "3. Get values for signing in\n",
    "4. Certificates and secrets -> Create a new application secret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points from the previous session:**\n",
    "* Pay attention to the requirements.txt as we only need to generate an ML Pipeline, azureml-sdk is the only package we need\n",
    "* Pay attention to the tests folder. Make sure you have at least 1 test under tests folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like always we import some packages related to the Azure ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the `secrets-config.json` file created in the previous notebook, to add more values to it for this notebook :\n",
    "```\n",
    "{\n",
    "    \"subscription_id\" : \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "    \"resource_group\" : \"...\",\n",
    "    \"workspace_name\" : \"...\",\n",
    "    \"workspace_region\" : \"...\",\n",
    "    \"tenant_id\" : \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "    \"application_id\" : \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "    \"app_secret\" : \"...\"\n",
    "}\n",
    "```\n",
    "Specifically, the values retrieved in the creation of the Service Principal process executed before should be added to the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('secrets-config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "tenant_id = config['tenant_id']\n",
    "application_id = config['application_id']\n",
    "subscription_id = config['subscription_id']\n",
    "app_secret = config['app_secret']\n",
    "resource_group = config['resource_group']\n",
    "workspace_name = config['workspace_name']\n",
    "workspace_region = config['workspace_region']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Service Principal Identity makes the code capable of accessing to our Azure Environment and Access the ML Workspace. In this case, it can create the Pipeline through the Build Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you download the file as Python and push the changes to the source repo. Now, we're ready to create our next gen Build Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "service_principal = ServicePrincipalAuthentication(\n",
    "        tenant_id=tenant_id,\n",
    "        service_principal_id=application_id,\n",
    "        service_principal_password=app_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(\n",
    "            name=workspace_name,\n",
    "            subscription_id=subscription_id,\n",
    "            resource_group=resource_group,\n",
    "            auth=service_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pointer to the default Blob storage.\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"mnist_datainput\",\n",
    "    path_on_datastore=\"mnist_datainput\")\n",
    "\n",
    "print(\"DataReference object created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CPU D2V2\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target_cpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target_cpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target_cpu.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target_cpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose a name for your cluster GPU NV6 1 node\n",
    "# cluster_name = \"gpucluster\"\n",
    "\n",
    "# try:\n",
    "#     compute_target_gpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "#     print('Found existing compute target.')\n",
    "# except ComputeTargetException:\n",
    "#     print('Creating a new compute target...')\n",
    "#     # CPU: Standard_D3_v2\n",
    "#     # GPU: Standard_NV6\n",
    "#     compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "#                                                            max_nodes=1,\n",
    "#                                                            min_nodes=1)\n",
    "\n",
    "#     # create the cluster\n",
    "#     compute_target_gpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "#     compute_target_gpu.wait_for_completion(show_output=True)\n",
    "\n",
    "# # use get_status() to get a detailed status for the current cluster. \n",
    "# print(compute_target_gpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mnist_data = PipelineData(\"processed_mnist_data\", datastore=def_blob_store)\n",
    "processed_mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                                          'numpy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source directory\n",
    "source_directory = 'DataExtraction'\n",
    "\n",
    "extractDataStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\", \n",
    "    arguments=[\"--output_extract\", processed_mnist_data],\n",
    "    outputs=[processed_mnist_data],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "\n",
    "print(\"Data Extraction Step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "source_directory = 'Training'\n",
    "est = TensorFlow(source_directory=source_directory,\n",
    "                 compute_target=compute_target_cpu,\n",
    "                 entry_script='train.py', \n",
    "                 use_gpu=False, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import EstimatorStep\n",
    "\n",
    "model_name = \"tf_mnist_pipeline_devops.model\"\n",
    "trainingStep = EstimatorStep(name=\"Training-Step\",\n",
    "                             estimator=est,\n",
    "                             estimator_entry_script_arguments=[\"--input_data_location\", processed_mnist_data,\n",
    "                                                               '--batch-size', 50,\n",
    "                                                               '--first-layer-neurons', 300,\n",
    "                                                               '--second-layer-neurons', 100,\n",
    "                                                               '--learning-rate', 0.01,\n",
    "                                                               \"--release_id\", 0,\n",
    "                                                               '--model_name', model_name],\n",
    "                             runconfig_pipeline_params=None,\n",
    "                             inputs=[processed_mnist_data],\n",
    "                             compute_target=compute_target_cpu)\n",
    "\n",
    "print(\"Model Training Step is Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source directory\n",
    "source_directory = 'RegisterModel'\n",
    "\n",
    "modelEvalReg = PythonScriptStep(\n",
    "    name=\"Evaluate and Register Model\",\n",
    "    script_name=\"evaluate_model.py\", \n",
    "    arguments=[\"--release_id\", 0,\n",
    "               '--model_name', model_name],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "\n",
    "modelEvalReg.run_after(trainingStep)\n",
    "print(\"Model Evaluation and Registration Step is Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "pipeline = Pipeline(workspace=ws, steps=[extractDataStep, trainingStep, modelEvalReg])\n",
    "pipeline_run = Experiment(ws, 'MNIST-Model-Training-Build-CI').submit(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True, raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name=\"MNIST-Pipeline-Created-At-Build-Pipeline\", \n",
    "                                                   description=\"Steps are: data preparation, training, model validation and model registration\", \n",
    "                                                   version=\"0.1\", \n",
    "                                                   continue_on_step_failure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
